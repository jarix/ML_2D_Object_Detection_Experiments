{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d289757d",
   "metadata": {},
   "source": [
    "# YOLO Object Detecion with Tensorflow Object Detection API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b473508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites and Dependencies:\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Dependencies\n",
    "# !pip install ultralytics\n",
    "# !pip install tf_slim \n",
    "# !pip install tensorflow-object-detection-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e3d80",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b87313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 10.7MB/s 0.6s.6s<0.0s0s\n"
     ]
    }
   ],
   "source": [
    "# Load the Model\n",
    "model = YOLO('yolov8n.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789eafbf",
   "metadata": {},
   "source": [
    "### Load an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8383b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = '../images/motorcycle_640x640.jpg'\n",
    "\n",
    "image = Image.open(IMAGE_PATH).convert('RGB')\n",
    "W, H = image.size\n",
    "image_np = np.array(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f2db2",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca6b6a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xyxy: [[     62.852      306.64      521.94      626.65]\n",
      " [     133.91      276.33      310.83      576.28]]\n",
      "scores: [    0.73594     0.38659]\n",
      "classes: [3 0]\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(source=image_np, conf=0.25, imgsz=640, verbose=False)\n",
    "r = results[0]\n",
    "\n",
    "xyxy = r.boxes.xyxy.cpu().numpy() if r.boxes is not None else np.empty((0, 4))\n",
    "scores = r.boxes.conf.cpu().numpy() if r.boxes is not None else np.empty((0,))\n",
    "classes = r.boxes.cls.cpu().numpy().astype(np.int64) if r.boxes is not None else np.empty((0,), dtype=np.int64)\n",
    "\n",
    "print(f\"xyxy: {xyxy}\")\n",
    "print(f\"scores: {scores}\")\n",
    "print(f\"classes: {classes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da36aec9",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25451113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow OD API style for visualization_utils\n",
    "# TF-OD expects boxes as [ymin, xmin, ymax, xmax] in *normalized* coordinates\n",
    "if xyxy.shape[0] > 0:\n",
    "    xmin = xyxy[:, 0] / W\n",
    "    ymin = xyxy[:, 1] / H\n",
    "    xmax = xyxy[:, 2] / W\n",
    "    ymax = xyxy[:, 3] / H\n",
    "    boxes_tf = np.stack([ymin, xmin, ymax, xmax], axis=1).astype(np.float32)\n",
    "else:\n",
    "    boxes_tf = np.empty((0, 4), dtype=np.float32)\n",
    "\n",
    "scores_tf = scores.astype(np.float32)\n",
    "# TF-OD label ids typically start at 1; YOLO classes start at 0.\n",
    "classes_tf = (classes + 1).astype(np.int32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10067c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'id': 1, 'name': 'person'}, 2: {'id': 2, 'name': 'bicycle'}, 3: {'id': 3, 'name': 'car'}, 4: {'id': 4, 'name': 'motorcycle'}, 5: {'id': 5, 'name': 'airplane'}, 6: {'id': 6, 'name': 'bus'}, 7: {'id': 7, 'name': 'train'}, 8: {'id': 8, 'name': 'truck'}, 9: {'id': 9, 'name': 'boat'}, 10: {'id': 10, 'name': 'traffic light'}, 11: {'id': 11, 'name': 'fire hydrant'}, 12: {'id': 12, 'name': 'stop sign'}, 13: {'id': 13, 'name': 'parking meter'}, 14: {'id': 14, 'name': 'bench'}, 15: {'id': 15, 'name': 'bird'}, 16: {'id': 16, 'name': 'cat'}, 17: {'id': 17, 'name': 'dog'}, 18: {'id': 18, 'name': 'horse'}, 19: {'id': 19, 'name': 'sheep'}, 20: {'id': 20, 'name': 'cow'}, 21: {'id': 21, 'name': 'elephant'}, 22: {'id': 22, 'name': 'bear'}, 23: {'id': 23, 'name': 'zebra'}, 24: {'id': 24, 'name': 'giraffe'}, 25: {'id': 25, 'name': 'backpack'}, 26: {'id': 26, 'name': 'umbrella'}, 27: {'id': 27, 'name': 'handbag'}, 28: {'id': 28, 'name': 'tie'}, 29: {'id': 29, 'name': 'suitcase'}, 30: {'id': 30, 'name': 'frisbee'}, 31: {'id': 31, 'name': 'skis'}, 32: {'id': 32, 'name': 'snowboard'}, 33: {'id': 33, 'name': 'sports ball'}, 34: {'id': 34, 'name': 'kite'}, 35: {'id': 35, 'name': 'baseball bat'}, 36: {'id': 36, 'name': 'baseball glove'}, 37: {'id': 37, 'name': 'skateboard'}, 38: {'id': 38, 'name': 'surfboard'}, 39: {'id': 39, 'name': 'tennis racket'}, 40: {'id': 40, 'name': 'bottle'}, 41: {'id': 41, 'name': 'wine glass'}, 42: {'id': 42, 'name': 'cup'}, 43: {'id': 43, 'name': 'fork'}, 44: {'id': 44, 'name': 'knife'}, 45: {'id': 45, 'name': 'spoon'}, 46: {'id': 46, 'name': 'bowl'}, 47: {'id': 47, 'name': 'banana'}, 48: {'id': 48, 'name': 'apple'}, 49: {'id': 49, 'name': 'sandwich'}, 50: {'id': 50, 'name': 'orange'}, 51: {'id': 51, 'name': 'broccoli'}, 52: {'id': 52, 'name': 'carrot'}, 53: {'id': 53, 'name': 'hot dog'}, 54: {'id': 54, 'name': 'pizza'}, 55: {'id': 55, 'name': 'donut'}, 56: {'id': 56, 'name': 'cake'}, 57: {'id': 57, 'name': 'chair'}, 58: {'id': 58, 'name': 'couch'}, 59: {'id': 59, 'name': 'potted plant'}, 60: {'id': 60, 'name': 'bed'}, 61: {'id': 61, 'name': 'dining table'}, 62: {'id': 62, 'name': 'toilet'}, 63: {'id': 63, 'name': 'tv'}, 64: {'id': 64, 'name': 'laptop'}, 65: {'id': 65, 'name': 'mouse'}, 66: {'id': 66, 'name': 'remote'}, 67: {'id': 67, 'name': 'keyboard'}, 68: {'id': 68, 'name': 'cell phone'}, 69: {'id': 69, 'name': 'microwave'}, 70: {'id': 70, 'name': 'oven'}, 71: {'id': 71, 'name': 'toaster'}, 72: {'id': 72, 'name': 'sink'}, 73: {'id': 73, 'name': 'refrigerator'}, 74: {'id': 74, 'name': 'book'}, 75: {'id': 75, 'name': 'clock'}, 76: {'id': 76, 'name': 'vase'}, 77: {'id': 77, 'name': 'scissors'}, 78: {'id': 78, 'name': 'teddy bear'}, 79: {'id': 79, 'name': 'hair drier'}, 80: {'id': 80, 'name': 'toothbrush'}}\n"
     ]
    }
   ],
   "source": [
    "names = model.model.names  # dict like {0: 'person', 1: 'bicycle', ...}\n",
    "# Create a label map dict that label_map_util understands\n",
    "label_map = {i + 1: {'id': i + 1, 'name': names[i]} for i in range(len(names))}\n",
    "category_index = label_map  # label_map_util.create_category_index would accept a similar structure\n",
    "\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0c71185",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FreeTypeFont' object has no attribute 'getsize'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mobject_detection\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m visualization_utils \u001b[38;5;28;01mas\u001b[39;00m viz_utils\n\u001b[32m      3\u001b[39m image_vis = image_np.copy()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mviz_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize_boxes_and_labels_on_image_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_vis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mboxes_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclasses_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscores_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcategory_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_normalized_coordinates\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_boxes_to_draw\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_score_thresh\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43magnostic_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mline_thickness\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m8\u001b[39m))\n\u001b[32m     18\u001b[39m plt.imshow(image_vis)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MyGithub\\ML_2D_Object_Detection_Experiments\\object_detection_with_tfod_api\\.venv\\Lib\\site-packages\\object_detection\\utils\\visualization_utils.py:735\u001b[39m, in \u001b[36mvisualize_boxes_and_labels_on_image_array\u001b[39m\u001b[34m(image, boxes, classes, scores, category_index, instance_masks, instance_boundaries, keypoints, use_normalized_coordinates, max_boxes_to_draw, min_score_thresh, agnostic_mode, line_thickness, groundtruth_box_visualization_color, skip_scores, skip_labels)\u001b[39m\n\u001b[32m    728\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instance_boundaries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    729\u001b[39m   draw_mask_on_image_array(\n\u001b[32m    730\u001b[39m       image,\n\u001b[32m    731\u001b[39m       box_to_instance_boundaries_map[box],\n\u001b[32m    732\u001b[39m       color=\u001b[33m'\u001b[39m\u001b[33mred\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    733\u001b[39m       alpha=\u001b[32m1.0\u001b[39m\n\u001b[32m    734\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m \u001b[43mdraw_bounding_box_on_image_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mymin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m    \u001b[49m\u001b[43mymax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthickness\u001b[49m\u001b[43m=\u001b[49m\u001b[43mline_thickness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisplay_str_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbox_to_display_str_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbox\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_normalized_coordinates\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_normalized_coordinates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keypoints \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    746\u001b[39m   draw_keypoints_on_image_array(\n\u001b[32m    747\u001b[39m       image,\n\u001b[32m    748\u001b[39m       box_to_keypoints_map[box],\n\u001b[32m    749\u001b[39m       color=color,\n\u001b[32m    750\u001b[39m       radius=line_thickness / \u001b[32m2\u001b[39m,\n\u001b[32m    751\u001b[39m       use_normalized_coordinates=use_normalized_coordinates)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MyGithub\\ML_2D_Object_Detection_Experiments\\object_detection_with_tfod_api\\.venv\\Lib\\site-packages\\object_detection\\utils\\visualization_utils.py:126\u001b[39m, in \u001b[36mdraw_bounding_box_on_image_array\u001b[39m\u001b[34m(image, ymin, xmin, ymax, xmax, color, thickness, display_str_list, use_normalized_coordinates)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Adds a bounding box to an image (numpy array).\u001b[39;00m\n\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03mBounding box coordinates can be specified in either absolute (pixel) or\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \u001b[33;03m    coordinates as absolute.\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    125\u001b[39m image_pil = Image.fromarray(np.uint8(image)).convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[43mdraw_bounding_box_on_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_pil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mymin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mymax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mthickness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_str_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m                           \u001b[49m\u001b[43muse_normalized_coordinates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m np.copyto(image, np.array(image_pil))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MyGithub\\ML_2D_Object_Detection_Experiments\\object_detection_with_tfod_api\\.venv\\Lib\\site-packages\\object_detection\\utils\\visualization_utils.py:182\u001b[39m, in \u001b[36mdraw_bounding_box_on_image\u001b[39m\u001b[34m(image, ymin, xmin, ymax, xmax, color, thickness, display_str_list, use_normalized_coordinates)\u001b[39m\n\u001b[32m    177\u001b[39m   font = ImageFont.load_default()\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# If the total height of the display strings added to the top of the bounding\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# box exceeds the top of the image, stack the strings below the bounding box\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# instead of above.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m display_str_heights = [\u001b[43mfont\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetsize\u001b[49m(ds)[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m display_str_list]\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# Each display_str has a top and bottom margin of 0.05x.\u001b[39;00m\n\u001b[32m    184\u001b[39m total_display_str_height = (\u001b[32m1\u001b[39m + \u001b[32m2\u001b[39m * \u001b[32m0.05\u001b[39m) * \u001b[38;5;28msum\u001b[39m(display_str_heights)\n",
      "\u001b[31mAttributeError\u001b[39m: 'FreeTypeFont' object has no attribute 'getsize'"
     ]
    }
   ],
   "source": [
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "image_vis = image_np.copy()\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "    image_vis,\n",
    "    boxes_tf,\n",
    "    classes_tf,\n",
    "    scores_tf,\n",
    "    category_index,\n",
    "    use_normalized_coordinates=True,\n",
    "    max_boxes_to_draw=100,\n",
    "    min_score_thresh=0.25,\n",
    "    agnostic_mode=False,\n",
    "    line_thickness=3\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(image_vis)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
